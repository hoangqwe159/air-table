# backend OA lyra

> [2 weeks] For ur task, I want u to do an **Airtable clone.** Use [https://create.t3.gg/](https://create.t3.gg/), deploy on vercel.
> 
> - Use TanStack table library for tables.
> - Don’t worry about the landing page
> - UI needs to match 1:1 in the main page with all the columns and cells, though you don’t have to implement all functionalities.
> - Be able to log in and create bases. In each base, I can create tables.
> - Be able to dynamically add columns.
>     - Text type columns are fine for now
> - Editing cells, and tabbing across everything should be smooth
> - Creating a new table will show default rows and columns. User fakerjs for data
> - I want to see a table w/ 15k rows and scroll down without lag
>     - Add a button I can click that will add 15k row to my table.
> - I want to be able to search across all the cells
> - I want to be able to create a 'view' of a table and save the following configurations
>     - Simple filters on columns: for both numbers (greater than, smaller than) and text (is not empty, is empty, contains, not contains equal to)
>     - Simple sorting on columns: for text A→Z, Z→A, for numbers do decreasing or increasing
>     - Can search through and hide/show columns.
> - Search, filter and sort has to be both in backend & frontend
> - Make sure there's loading state

Functional requirement

- create base, table & column
- edit cells
- seed a table with 15k rows
- search across all cells
- create view
- filter & sort on text/number columns

functional requirements not explicitly mentioned

- can reorder/delete/insert row/column freely (no restriction that you can only insert to the end)

non-functional requirements I assumed

- value availability over consistency ⇒ okay with eventual consistency
- high write/read throughput for table rows
- able to handle large data set (≥ 100k rows per table)
- low latency

Here’s the simplified architecture

![image.png](backend%20OA%20lyra%201641aaf7556b809d8445d54c0f7298c2/image.png)

Components for a production-ready backend like

- api gateway & load balancer
- scaling group / ECS / k8s
- observability
- db read / write separation

and so many more are assumed and not displayed for brevity

Simplified flow

- when user CRUD base, table, column, view ⇒ query MySQL
- when user CRUD rows data ⇒ DynamoDB
- rows data are indexed in ElasticSearch to enable fast search/filter/sort/aggregation etc.,
    - DynamoDB events are streamed to ElasticSearch for (re)-indexing
- to support smooth scrolling, when client fetch rows
    - if there’s no search/filter/sort ⇒ query DynamoDB ⇒ return row data
    - if there is ⇒ query ElasticSearch ⇒ ElasticSearch will return results (that can be used to identify the row in DynamoDB) ⇒ query DynamoDB
        - either way we’ll use pagination (built-in for DynamoDB or Search After API for ElasticSearch)
            - if there are a lot of rows & we’re concerned with DynamoDB’s limitation with deep pagination ⇒ just use Search After API from the beginning
- use Redis when possible to improve performance

Data Modeling & Decision

- data about base, table, column, view per table are relational ⇒ MySQL
- data about rows is stored in DynamoDB for high throughput
    - data of a single row is stored in a single DynamoDB row (elaborate later)

- MySQL
    
    ```sql
    -- table base
    CREATE TABLE IF NOT EXISTS base (
        id CHAR(36) PRIMARY KEY,            -- UUID is stored as CHAR(36)
        name VARCHAR(50) NOT NULL           -- base name
    );
    
    CREATE TABLE IF NOT EXISTS table_info (
        id CHAR(36) PRIMARY KEY,            -- UUID is stored as CHAR(36)
        base_id CHAR(36) NOT NULL,          -- FK referencing base.id
        name VARCHAR(50) NOT NULL,          -- table name
        CONSTRAINT base_id_fk FOREIGN KEY (base_id) REFERENCES base(id) ON DELETE CASCADE,
        INDEX idx_table_info_base_id (base_id)  -- benefit query "given base_id, find all tables"
    );
    
    -- Create the column_data_type enum
    CREATE TABLE IF NOT EXISTS column_data_type (
        data_type VARCHAR(15) NOT NULL UNIQUE
    );
    
    -- Insert enum values into the column_data_type table
    INSERT IGNORE INTO column_data_type (data_type) VALUES ('text'), ('number');
    
    -- table column_info (column is a reserved word)
    CREATE TABLE IF NOT EXISTS column_info (
        table_id CHAR(36) NOT NULL,         -- FK referencing table_info.id
        col_number INT NOT NULL,       -- column number
        data_type VARCHAR(10) NOT NULL,     -- data type (text or number, stored as string)
        name VARCHAR(50) NOT NULL,          -- column name
        PRIMARY KEY (table_id, col_number), -- composite primary key
        CONSTRAINT table_id_fk FOREIGN KEY (table_id) REFERENCES table_info(id) ON DELETE CASCADE,
        CONSTRAINT data_type_fk FOREIGN KEY (data_type) REFERENCES column_data_type(data_type) ON DELETE RESTRICT
    );
    
    -- table view_info
    CREATE TABLE IF NOT EXISTS view_info (
        view_id CHAR(36) PRIMARY KEY NOT NULL UNIQUE,         -- Primary key as UUID
        table_id CHAR(36) NOT NULL,                          -- Foreign key referencing table_info.id
        view_content JSON NOT NULL,                         -- JSON content
        view_name VARCHAR(255) DEFAULT NULL,                -- Optional view name, defaults to NULL
        CONSTRAINT table_id_fk FOREIGN KEY (table_id) REFERENCES table_info(id) ON DELETE CASCADE,
        INDEX idx_table_id (table_id)                       -- benefit query, "given table_id, find all views"
    );
    ```
    
- DynamoDB
    
    
    | `table_id` | partition key |
    | --- | --- |
    | `row_number` | sort key |
    | `row_data` | array of JSON |

- example state of DB:
    
    table `base`
    
    | `id` | `name` |
    | --- | --- |
    | ebc25034-c020-11ef-8b2d-e3a48ccf6aa4 | my base |
    
    table `table_info`
    
    | `id` | `base_id` | `name` |
    | --- | --- | --- |
    | b6bc5e8d-f0e2-4d63-b49d-ee28c5188d28 | ebc25034-c020-11ef-8b2d-e3a48ccf6aa4 | my table |
    |  |  |  |
    
    table `column_info`
    
    | `table_id` | `col_number` | `data_type` | `name` |
    | --- | --- | --- | --- |
    | b6bc5e8d-f0e2-4d63-b49d-ee28c5188d28 | 1000 | text | Name |
    | b6bc5e8d-f0e2-4d63-b49d-ee28c5188d28 | 2000 | text | University |
    | b6bc5e8d-f0e2-4d63-b49d-ee28c5188d28 | 3000 | text | Occupation |
    | b6bc5e8d-f0e2-4d63-b49d-ee28c5188d28 | 4000 | number | Age |
    | b6bc5e8d-f0e2-4d63-b49d-ee28c5188d28 | 5000 | number | Credit Score |
    
    table `view_info`
    
    | `view_id` | `table_id` | `view_content` | `view_name` |
    | --- | --- | --- | --- |
    | c7496b69-503d-4578-9187-a32c84775b79 | b6bc5e8d-f0e2-4d63-b49d-ee28c5188d28 | a JSON representing the filter/search/sort conditions | Company’s engineers by age |
    | 14e78155-91c9-4bcd-b5ba-14acb4a9f8bd | b6bc5e8d-f0e2-4d63-b49d-ee28c5188d28 | a JSON representing the filter/search/sort conditions | People who went to Harvard with Credit score ≥ 500 |
    
    table `row_data` (DynamoDB)
    
    | `table_id` | `row_number` | `row_data` |
    | --- | --- | --- |
    | b6bc5e8d-f0e2-4d63-b49d-ee28c5188d28 | 100,000 | [
        { colNumber: 1000, value: "Do Viet Hoang" },
        { colNumber: 2000, value: "Queensland University of Technology" },
        { colNumber: 3000, value: "Software Engineering" },
        { colNumber: 4000, value: "24" },
        { colNumber: 5000, value: 600 },
    ] |
    | b6bc5e8d-f0e2-4d63-b49d-ee28c5188d28 | 100,001 | [
        { colNumber: 1000, value: "Sarah Johnson" },
        { colNumber: 2000, value: "University of Melbourne" },
        { colNumber: 3000, value: "Data Scientist" },
        { colNumber: 4000, value: "28" },
        { colNumber: 5000, value: 750 },
    ] |
    | b6bc5e8d-f0e2-4d63-b49d-ee28c5188d28 | 200,000 | [
        { colNumber: 1000, value: "Michael Chen" },
        { colNumber: 2000, value: "University of Sydney" },
        { colNumber: 3000, value: "Product Manager" },
        { colNumber: 4000, value: "31" },
        { colNumber: 5000, value: 680 },
    ] |
    | b6bc5e8d-f0e2-4d63-b49d-ee28c5188d28 | 300,000 | [
        { colNumber: 1000, value: "Emma Wilson" },
        { colNumber: 2000, value: "RMIT University" },
        { colNumber: 3000, value: "UX Designer" },
        { colNumber: 4000, value: "26" },
        { colNumber: 5000, value: 720 },
    ] |
    | b6bc5e8d-f0e2-4d63-b49d-ee28c5188d28 | 400,000 | [
        { colNumber: 1000, value: "James Taylor" },
        { colNumber: 2000, value: "Monash University" },
        { colNumber: 3000, value: "Software Developer" },
        { colNumber: 4000, value: "29" },
        { colNumber: 5000, value: 695 },
    ] |

Expected questions:

- why not use PostgresSQL/MySQL/DynamoDB for filter/search/sort? why use ElasticSearch?
    - those DB supports filter/search/sort to some extend, but no where near as good as ElasticSearch
    - deep pagination degrades DB’s performance, ElasticSearch can handle it without a sweat via Search After API
    - DB are already busy keeping up with the high read/write throughput + separation of concerns is nice
        - technically we can implement writer - reader instances pattern, where reader instances will handle all the heavy lifting for filter/search/sort
            - but again, it’s not worth it given ElasticSearch can do it better

- column & table `column_info`
    - why use `col_number` ? why in DynamoDB’s `row_data` we don’t store `{fieldName: val}`?
        - if user rename a column, we only need to update a single DB record
        - use `col_number` because columns in a table has order
    - why store `data_type`? why store the table’s schema?
        - need this if frontend wishes to display each column’s data type
        - so that before upserting in DynamoDB, we can verify if row data provided client matches the format or not
        - Elastic will throw error if the document being indexed doesn’t match the declared mapping
            - we don’t want to successfully upsert Dynamo then fail to index
            - DynamoDB and Elastic will be out of sync

- why use DynamoDB? why not use MySQL?
    - row data isn’t relational
    - better scalability, compared to relational DB which are mostly designed for scaling vertically; technically can scale horizontally but it’s complicated
    - better write throughput compared to relational DB
        - why better write throughput? DynamoDB still has to maintain indexes (partition key & sort key)
            - while DynamoDB still has to maintain indexes, it doesn’t have to
                - handle ACID transactions
                - maintain relations/contraints
                - locking mechanism
                    - ⇒ support write throughput better than relational DB
        - high write throughput works well for seeding database function (add 15k random rows)
    - choice of partition key?
        - use `table_id` as partition key so that all rows in a single table will stay in a single partition ⇒ better querying

- use cases for redis?
    - depends on how read/write heavy the operation is/how frequently data change, can use redis to store
        - base name by base id
        - tables by base id
        - views by table id
        - columns by table id
        - fetch row requests based on view

- how to handle row order change when insert in the middle/delete/drag to move order
    - in other words, if row/column number is 1 - 2 - 3 - 4 - 5 - 1,000,000
        - if we move row 2 to the end/insert a row between 2 and 3/delete row 2
            - row number of row 3 → 1,000,000 will have to change → huge DB update for simple operation
        - solution:
            - we number row as 100,000 - 200,000 - 300,000 - 100,000,000,000 (no integer overflow, 10^11 is much smaller than 64 bit integer)
            - if row 200,000 is moved to the end ⇒ update row number to be 100,000,000,001 ⇒ only 1 DB update required
            - if row a row is inserted between 200,000 and 300,000 ⇒ only 1 DB update to register new row as 200,001
            - if row 200,000 is deleted ⇒ only 1 DB update is needed to delete that row, table’s row order remains valid
            - if 100,000 rows are added after row 200,000
                - need 100,000 updates to register those rows
                - need 1 update to move row 300,000 to be 300,001
            - in short, this solution scales well

I acknowledge that there are room for improvements in my design (add queueing to handle high load etc,), but given requirement & time to implement/plan, this works as a solid starting point